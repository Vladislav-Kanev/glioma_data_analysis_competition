{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Glioma Grading Clinical and Mutation Features"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from pathlib import Path\n",
                "import shutil\n",
                "import os\n",
                "\n",
                "import pandas as pd\n",
                "import matplotlib.font_manager\n",
                "import warnings\n",
                "warnings.filterwarnings(\"ignore\")\n",
                "\n",
                "from src.data_processing import process_data, normalize_dataset\n",
                "from src.feature_estimators import get_feature_estimator, select_by_correlation_value, concat_important_features\n",
                "from src.visualize import make_2d_representation"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Data processing\n",
                "---"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "DATASETS_PATH = Path('datasets')\n",
                "train_dataset_path = DATASETS_PATH / 'train.csv'\n",
                "test_dataset_path = DATASETS_PATH / 'test.csv'"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "train_dataset = pd.read_csv(train_dataset_path, index_col=0)\n",
                "test_dataset = pd.read_csv(test_dataset_path, index_col=0)\n",
                "\n",
                "train_dataset.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Column Analysis\n",
                "\n",
                "* The `Grade` column is our target.\n",
                "\n",
                "* The column `Primary_Diagnosis` has 5 unique values, we will encode it with `LabelEncoder`.\n",
                "\n",
                "* Column `Case_ID` represents unique id of case, we will remove from our dataset.\n",
                "\n",
                "* The following columns represent the gen mutations.\n",
                "    ```text\n",
                "    IDH1, TP53, ATRX, PTEN, EGFR, CIC, MUC16, PIK3CA,\n",
                "    NF1, PIK3R1, FUBP1, RB1, NOTCH1, BCOR, CSMD3, SMARCA4,\n",
                "    GRIN2A, IDH2, FAT4, PDGFRA\n",
                "    ```\n",
                "    They might be only `MUTATED` or `NOT_MUTATED`, so we will encode it with `LabelEncoder`.\n",
                "\n",
                "* The binary type column `Gender` will be encoded with `LabelEncoder` too.\n",
                "\n",
                "* The `Age_at_diagnosis` column has a string representation of date. We will convert it into the numeric type.\n",
                "\n",
                "### Missing Data\n",
                "\n",
                "We detect that 4 cases has no `Age_at_diagnosis` data. We decided to remove them from training set."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "encoder = 'Label'\n",
                "\n",
                "train_dataset = process_data(train_dataset, encoder=encoder, target='Grade')\n",
                "test_dataset = process_data(test_dataset, encoder=encoder)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "2d Representation of our data is presented at the figure below"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "figure = make_2d_representation(train_dataset)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Feature selection\n",
                "---\n",
                "\n",
                "To investigate the most valuable features we decided to calculate some correlation metrics."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "data_for_feature_analysis = train_dataset.copy()\n",
                "training_data = normalize_dataset(train_dataset.drop(columns=['Grade']))\n",
                "validation_data = normalize_dataset(test_dataset)\n",
                "targets = data_for_feature_analysis['Grade']"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Mutual information\n",
                "Mutual information is a lot like correlation in that it measures a relationship between two quantities. The advantage of mutual information is that it can detect any kind of relationship, while correlation only detects linear relationships.\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "correlation_estimator = get_feature_estimator(training_data, targets, method='mutual_info-classification')\n",
                "mutual_important_values = select_by_correlation_value(correlation_estimator, min_score=0.2)\n",
                "mutual_important_values"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The most valuable features are:\n",
                "```text\n",
                "Primary_Diagnosis, IDH1, Age_at_diagnosis\n",
                "```\n",
                "\n",
                "The least valuable features are:\n",
                "```text\n",
                "Gender, BCOR, FAT4, PIK3CA, Race, GRIN2A, PIK3R1\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Pearson's \n",
                "\n",
                "The Pearson correlation measures the strength of the linear relationship between two variables."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "correlation_estimator = get_feature_estimator(training_data, targets, method='pearson')\n",
                "pearson_important_values = select_by_correlation_value(correlation_estimator, min_score=0.2)\n",
                "pearson_important_values"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "The most valuable features are\n",
                "```text\n",
                "IDH1, Age_at_diagnosis, PTEN, ATRX, CIC\n",
                "```\n",
                "\n",
                "The least valuable features are:\n",
                "```text\n",
                "BKOR, PIK3CA, FAT4\n",
                "```"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Selected Features\n",
                "\n",
                "We decided to automatically select features if their correlation metric is greater than 0.2"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "keep_columns = concat_important_features(pearson_important_values, mutual_important_values)\n",
                "training_data = training_data[keep_columns]\n",
                "validation_data = test_dataset[keep_columns]\n",
                "training_data"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Experiments with model"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.models import logreg_classifier, catboost_classifier, rf_classifier\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import f1_score\n",
                "\n",
                "x_train, x_test, y_train, y_test = train_test_split(training_data, targets, test_size=0.3, random_state=42)\n",
                "\n",
                "model_map = {\n",
                "    'Logistic Regression Classifier': logreg_classifier,\n",
                "    'CatBoost Classifier': catboost_classifier,\n",
                "    'Random Forest' : rf_classifier,\n",
                "}\n",
                "results_path = Path('results')\n",
                "if (os.path.exists(results_path)):\n",
                "    shutil.rmtree(results_path)\n",
                "results_path.mkdir(exist_ok=True)\n",
                "\n",
                "for model_name, model in model_map.items():\n",
                "    classifier = model(x_train, y_train)\n",
                "    \n",
                "    prediction = classifier.predict(x_test)\n",
                "    score = f1_score(y_test, prediction)\n",
                "    print(f'Model {model_name}: {score}')\n",
                "\n",
                "    model_file_name = model_name.replace(' ', '_').lower()\n",
                "    model_result = results_path / f'model_{model_file_name}_{score: .2f}.csv'\n",
                "    val_prediction = classifier.predict(validation_data)\n",
                "    val_prediction = [int(not value) for value in val_prediction]\n",
                "    pd.DataFrame(zip(range(len(val_prediction)), val_prediction),\n",
                "                 columns=['Id', 'Grade']).to_csv(model_result, index=False)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from src.models import voting_classifier\n",
                "ensemble_model = voting_classifier(x_train, y_train)\n",
                "\n",
                "prediction = classifier.predict(x_test)\n",
                "score = f1_score(y_test, prediction)\n",
                "\n",
                "print(f'voting classifier score: {score}')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.6"
        },
        "orig_nbformat": 4
    },
    "nbformat": 4,
    "nbformat_minor": 2
}
